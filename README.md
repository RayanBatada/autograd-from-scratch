# Autograd from Scratch

This project implements a simple reverse-mode automatic differentiation (autograd) engine from scratch in Python. Itâ€™s inspired by Andrej Karpathyâ€™s [micrograd](https://github.com/karpathy/micrograd), and is designed to help understand the inner workings of neural networks, backpropagation, and gradient-based optimization â€” all without using external machine learning libraries.

## ğŸš€ Features

- Scalar `Value` class with automatic gradient tracking
- Operator overloading for math operations (`+`, `-`, `*`, `**`, etc.)
- Backpropagation via `.backward()` method
- Intuitive and readable implementation using object-oriented Python
- (Optional) Neural network example using the custom autograd engine

## ğŸ“‚ Files

- `Autograd.ipynb`: A step-by-step Jupyter Notebook explaining and implementing the autograd engine.

## ğŸ§  Concepts Covered

- Reverse-mode automatic differentiation
- Computational graphs
- Chain rule & gradient propagation
- Object-oriented programming
- Basics of neural networks

## â–¶ï¸ Running the Notebook

1. Clone the repo:
    ```bash
    git clone https://github.com/RayanBatada/autograd-from-scratch.git
    cd autograd-from-scratch
    ```

2. Launch Jupyter:
    ```bash
    jupyter notebook Autograd.ipynb
    ```
